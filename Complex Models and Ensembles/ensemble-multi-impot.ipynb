{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble with multiple mixed inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import layers, models, Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting the directory path for the spectograms\n",
    "import os\n",
    "images = os.listdir('Data/images_original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'blues',\n",
       " 'classical',\n",
       " 'country',\n",
       " 'disco',\n",
       " 'hiphop',\n",
       " 'jazz',\n",
       " 'metal',\n",
       " 'pop',\n",
       " 'reggae',\n",
       " 'rock']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking the path is set properly\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting the  folder names inside the images folder\n",
    "genre_folders = [f for f in os.listdir('Data/images_original') if not f.startswith('.')]\n",
    "\n",
    "##creating  a datarfame and setting it to the folder folder path\n",
    "images_df = pd.DataFrame({\n",
    "    'Names': genre_folders,\n",
    "    'Path': [os.path.join('Data/images_original', genre) for genre in genre_folders]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues</td>\n",
       "      <td>Data/images_original\\blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classical</td>\n",
       "      <td>Data/images_original\\classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>country</td>\n",
       "      <td>Data/images_original\\country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disco</td>\n",
       "      <td>Data/images_original\\disco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hiphop</td>\n",
       "      <td>Data/images_original\\hiphop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jazz</td>\n",
       "      <td>Data/images_original\\jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>metal</td>\n",
       "      <td>Data/images_original\\metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pop</td>\n",
       "      <td>Data/images_original\\pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reggae</td>\n",
       "      <td>Data/images_original\\reggae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rock</td>\n",
       "      <td>Data/images_original\\rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Names                            Path\n",
       "0      blues      Data/images_original\\blues\n",
       "1  classical  Data/images_original\\classical\n",
       "2    country    Data/images_original\\country\n",
       "3      disco      Data/images_original\\disco\n",
       "4     hiphop     Data/images_original\\hiphop\n",
       "5       jazz       Data/images_original\\jazz\n",
       "6      metal      Data/images_original\\metal\n",
       "7        pop        Data/images_original\\pop\n",
       "8     reggae     Data/images_original\\reggae\n",
       "9       rock       Data/images_original\\rock"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking the new dataframe works\n",
    "images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting the path for the csv file\n",
    "csv_path = 'Data/features_30_sec.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data/features_30_sec.csv'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking the path is set properly\n",
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting the size of the picture \n",
    "image_size = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading the csv file\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dropping the length column on the csv file\n",
    "df = df.drop(columns=[\"length\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##extracting the genre \n",
    "df['label'] = df['filename'].apply(lambda x: x.split('.')[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dropping the  columns we dont need \n",
    "columns_to_drop = ['filename', 'length', 'label']\n",
    "X_tab = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "##scaling the remaining columns using standardscaler\n",
    "scaler = StandardScaler()\n",
    "X_tab_scaled = scaler.fit_transform(X_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##converting  the labels in the datafram to numbers \n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "##converting the numbers to one-hotencoding \n",
    "y_categorical = to_categorical(y)\n",
    "\n",
    "##checking how mani classes we have so like our genres\n",
    "num_genres = y_categorical.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loadingthe images on a list of filenames and then we return them as a numoy array\n",
    "def load_spectrogram_images(filenames, base_dir, target_size):\n",
    "    data = []\n",
    "    for name in filenames:\n",
    "        ##extracting the genre and the image so like  blues000000.png wesplitinto blue sand the number\n",
    "        genre = name.split('.')[0]          \n",
    "        img_name = name.split('.')[1] + '.png' \n",
    "\n",
    "        ##bthe full path to the spectogram image \n",
    "        img_path = os.path.join(base_dir, genre, img_name)\n",
    "        \n",
    "        ##if the file existsweload it conver it into an array and scale the pixel values\n",
    "        if os.path.exists(img_path):\n",
    "            img = load_img(img_path, target_size=target_size)\n",
    "            img = img_to_array(img) / 255.0\n",
    "            data.append(img)\n",
    "        else:\n",
    "            ##if  the file doesn´t exist i throw a warning and i add an array with 0s to store the place like to keep running  so we make like a fake image prop with the same shape\n",
    "            print(f\"Warning: {img_path} not found. Skipping.\")\n",
    "            data.append(np.zeros((*target_size, 3))) \n",
    "            \n",
    "    ##then i just get the array back from the listof images\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting the image directory for the model later\n",
    "image_dir = 'Data/images_original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading the spectogram images and resizing them\n",
    "X_img = load_spectrogram_images(df['filename'].values, image_dir, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##splitting into train and testing set\n",
    "X_img_train, X_img_test, X_tab_train, X_tab_test, y_train, y_test = train_test_split(\n",
    "    X_img, X_tab_scaled, y_categorical, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating the cnn model\n",
    "image_input = Input(shape=(image_size[0], image_size[1], 3))\n",
    "##layer 1\n",
    "convolayer1 = layers.Conv2D(32, (3,3), activation='relu')(image_input)\n",
    "convolayer1 = layers.MaxPooling2D((2,2)) (convolayer1)\n",
    "\n",
    "##adding a second layer\n",
    "convolayer2 = layers.Conv2D(32, (3,3), activation='relu')(convolayer1)\n",
    "convolayer2 = layers.MaxPooling2D((2,2)) (convolayer2)\n",
    "convolayer2 = layers.Flatten()(convolayer2)\n",
    "convolayer2 = layers.Dense(64, activation='relu')(convolayer2)\n",
    "\n",
    "image_branch = models.Model(inputs=image_input, outputs=convolayer2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defining the input layer for the table daya using the number of columns \n",
    "table_input = Input(shape=(X_tab_scaled.shape[1],))\n",
    "\n",
    "##passing the table data through 2 connected dense layers\n",
    "dense1 = layers.Dense(64, activation='relu')(table_input)\n",
    "dense2 = layers.Dense(32, activation='relu')(dense1)\n",
    "\n",
    "#creating a model object called table branch after running it through the layers\n",
    "table_branch = models.Model(inputs=table_input, outputs=dense2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##combining the csv and the images\n",
    "combined = layers.concatenate([image_branch.output, table_branch.output])\n",
    "densecomb = layers.Dense(64, activation='relu')(combined)\n",
    "densecomb = layers.Dropout(0.5)(densecomb)\n",
    "densecomb = layers.Dense(num_genres, activation='softmax')(densecomb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##building the combined model from the 2 branches above with an optimizer adam\n",
    "model = models.Model(inputs=[image_branch.input, table_branch.input], outputs=densecomb)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 130ms/step - accuracy: 0.1481 - loss: 2.3342 - val_accuracy: 0.3700 - val_loss: 1.9013\n",
      "Epoch 2/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.3053 - loss: 1.8940 - val_accuracy: 0.4800 - val_loss: 1.6329\n",
      "Epoch 3/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.4168 - loss: 1.6423 - val_accuracy: 0.5600 - val_loss: 1.4035\n",
      "Epoch 4/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.5115 - loss: 1.4128 - val_accuracy: 0.6200 - val_loss: 1.1907\n",
      "Epoch 5/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.5920 - loss: 1.1269 - val_accuracy: 0.6650 - val_loss: 1.0253\n",
      "Epoch 6/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.6696 - loss: 1.0064 - val_accuracy: 0.7050 - val_loss: 0.8965\n",
      "Epoch 7/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7066 - loss: 0.9115 - val_accuracy: 0.7250 - val_loss: 0.8006\n",
      "Epoch 8/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7545 - loss: 0.7346 - val_accuracy: 0.7700 - val_loss: 0.6912\n",
      "Epoch 9/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.8024 - loss: 0.6011 - val_accuracy: 0.8250 - val_loss: 0.5578\n",
      "Epoch 10/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.8398 - loss: 0.4683 - val_accuracy: 0.8300 - val_loss: 0.5320\n",
      "Epoch 11/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.8421 - loss: 0.4477 - val_accuracy: 0.8300 - val_loss: 0.5267\n",
      "Epoch 12/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.8539 - loss: 0.4620 - val_accuracy: 0.8400 - val_loss: 0.4595\n",
      "Epoch 13/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.8966 - loss: 0.2948 - val_accuracy: 0.8700 - val_loss: 0.4081\n",
      "Epoch 14/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9172 - loss: 0.2242 - val_accuracy: 0.8950 - val_loss: 0.3527\n",
      "Epoch 15/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9420 - loss: 0.1813 - val_accuracy: 0.8650 - val_loss: 0.4499\n",
      "Epoch 16/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9446 - loss: 0.1836 - val_accuracy: 0.9050 - val_loss: 0.3664\n",
      "Epoch 17/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9596 - loss: 0.1404 - val_accuracy: 0.8750 - val_loss: 0.4132\n",
      "Epoch 18/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9681 - loss: 0.1046 - val_accuracy: 0.8800 - val_loss: 0.4063\n",
      "Epoch 19/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9806 - loss: 0.0886 - val_accuracy: 0.8850 - val_loss: 0.3866\n",
      "Epoch 20/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9808 - loss: 0.0915 - val_accuracy: 0.8800 - val_loss: 0.4694\n",
      "Epoch 21/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9715 - loss: 0.1097 - val_accuracy: 0.8850 - val_loss: 0.3774\n",
      "Epoch 22/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9766 - loss: 0.0810 - val_accuracy: 0.8950 - val_loss: 0.5103\n",
      "Epoch 23/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9775 - loss: 0.0660 - val_accuracy: 0.9250 - val_loss: 0.4546\n",
      "Epoch 24/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9793 - loss: 0.0721 - val_accuracy: 0.9050 - val_loss: 0.4825\n",
      "Epoch 25/25\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9920 - loss: 0.0431 - val_accuracy: 0.9300 - val_loss: 0.4362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x223ff555970>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##training the model\n",
    "model.fit([X_img_train, X_tab_train], y_train,\n",
    "           validation_data=([X_img_test, X_tab_test], y_test),\n",
    "           epochs=25,\n",
    "           batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9272 - loss: 0.4276\n",
      "accuracy: 93.00%\n"
     ]
    }
   ],
   "source": [
    "##printing out the accuracy \n",
    "loss, accuracy = model.evaluate([X_img_test, X_tab_test], y_test)\n",
    "print(f\"accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimusic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
